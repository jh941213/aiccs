"""
í™”ì ë¶„ë¦¬ ì„œë¹„ìŠ¤ (Speaker Diarization)
"""
from pathlib import Path
from typing import List, Tuple

from pyannote.audio import Pipeline
from loguru import logger
import soundfile as sf
import torch
import numpy as np

from app.core.config import settings


class DiarizationService:
    """í™”ì ë¶„ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self, hf_token: str = None):
        """
        ì´ˆê¸°í™”

        Args:
            hf_token: Hugging Face ì•¡ì„¸ìŠ¤ í† í°
                    (https://huggingface.co/settings/tokensì—ì„œ ìƒì„±)
        """
        self.pipeline = None
        self.hf_token = hf_token or settings.hf_token
        self._pipeline_loaded = False

    def load_pipeline(self):
        """í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if self._pipeline_loaded:
            return

        if not self.hf_token:
            raise ValueError(
                "Hugging Face í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                "https://huggingface.co/settings/tokensì—ì„œ ìƒì„± í›„ "
                ".env íŒŒì¼ì— HF_TOKEN ì„¤ì •í•˜ì„¸ìš”."
            )

        logger.info("ğŸ”„ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì¤‘...")

        try:
            # pyannote community-1: ìµœì‹  ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸
            # ìµœì‹  pyannote.audioëŠ” í™˜ê²½ ë³€ìˆ˜ HF_TOKENì„ ìë™ìœ¼ë¡œ ì‚¬ìš©
            self.pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-community-1"
            )

            # GPU ì‚¬ìš© ì„¤ì • (Mac MPS ì§€ì›)
            if torch.backends.mps.is_available():
                self.pipeline.to(torch.device("mps"))
                logger.info("âœ… MPS(Apple Silicon)ë¡œ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            elif torch.cuda.is_available():
                self.pipeline.to(torch.device("cuda"))
                logger.info("âœ… CUDA GPUë¡œ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.info("âœ… CPUë¡œ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            self._pipeline_loaded = True

        except Exception as e:
            logger.error(f"âŒ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def unload_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì–¸ë¡œë“œ (GPU ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.pipeline is not None:
            del self.pipeline
            self.pipeline = None
            self._pipeline_loaded = False

            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info("âœ… í™”ì ë¶„ë¦¬ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")

    def diarize(
        self,
        audio_path: Path,
        num_speakers: int = None,
        min_speakers: int = None,
        max_speakers: int = None,
    ) -> List[Tuple[float, float, str]]:
        """
        í™”ì ë¶„ë¦¬ ìˆ˜í–‰

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            num_speakers: ì •í™•í•œ í™”ì ìˆ˜ (ì•Œê³  ìˆëŠ” ê²½ìš°)
            min_speakers: ìµœì†Œ í™”ì ìˆ˜
            max_speakers: ìµœëŒ€ í™”ì ìˆ˜

        Returns:
            [(ì‹œì‘ì‹œê°„(ì´ˆ), ì¢…ë£Œì‹œê°„(ì´ˆ), í™”ìID), ...]
        """
        if not self._pipeline_loaded:
            self.load_pipeline()

        logger.info(f"ğŸ¤ í™”ì ë¶„ë¦¬ ì‹œì‘: {audio_path.name}")

        try:
            # í™”ì ìˆ˜ ì„¤ì •
            kwargs = {}
            if num_speakers is not None:
                kwargs["num_speakers"] = num_speakers
            else:
                if min_speakers is not None:
                    kwargs["min_speakers"] = min_speakers
                if max_speakers is not None:
                    kwargs["max_speakers"] = max_speakers

            # audio_in_memory í˜•ì‹ìœ¼ë¡œ ì „ë‹¬ (torchcodec ë¬¸ì œ ìš°íšŒ)
            waveform, sample_rate = sf.read(str(audio_path))
            if waveform.ndim == 1:
                # Mono: (time,) -> (1, time)
                waveform = waveform.reshape(1, -1)
            else:
                # Stereo: (time, channel) -> (channel, time)
                waveform = waveform.T

            audio_in_memory = {
                "waveform": torch.from_numpy(waveform).float(),
                "sample_rate": sample_rate
            }

            # community-1: í™”ì ë¶„ë¦¬ ì‹¤í–‰
            output = self.pipeline(audio_in_memory, **kwargs)

            # community-1 API: ë” ê°„ë‹¨í•œ iteration
            segments = []
            for turn, speaker in output.speaker_diarization:
                segments.append((turn.start, turn.end, speaker))

            logger.info(f"âœ… í™”ì ë¶„ë¦¬ ì™„ë£Œ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

            # í™”ì ì •ë³´ ë¡œê¹…
            speakers = set(seg[2] for seg in segments)
            logger.info(f"ğŸ“Š ê°ì§€ëœ í™”ì ìˆ˜: {len(speakers)}")

            return segments

        except Exception as e:
            logger.error(f"âŒ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    def merge_with_transcript(
        self,
        diarization_segments: List[Tuple[float, float, str]],
        whisper_segments: List[Tuple[str, str, str]],
    ) -> List[Tuple[str, str, str, str]]:
        """
        í™”ì ë¶„ë¦¬ ê²°ê³¼ì™€ Whisper STT ê²°ê³¼ ë³‘í•©

        Args:
            diarization_segments: [(ì‹œì‘(ì´ˆ), ì¢…ë£Œ(ì´ˆ), í™”ì), ...]
            whisper_segments: [(ì‹œì‘ì‹œê°, ì¢…ë£Œì‹œê°, í…ìŠ¤íŠ¸), ...]

        Returns:
            [(ì‹œì‘ì‹œê°, ì¢…ë£Œì‹œê°, í™”ì, í…ìŠ¤íŠ¸), ...]
        """
        logger.info("ğŸ”„ í™”ì ì •ë³´ì™€ STT ê²°ê³¼ ë³‘í•© ì¤‘...")

        merged_segments = []

        for srt_start, srt_end, text in whisper_segments:
            # SRT íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
            start_sec = self._timestamp_to_seconds(srt_start)
            end_sec = self._timestamp_to_seconds(srt_end)

            # í•´ë‹¹ ì‹œê°„ëŒ€ì˜ í™”ì ì°¾ê¸° (ì¤‘ê°„ ì§€ì  ê¸°ì¤€)
            mid_sec = (start_sec + end_sec) / 2
            speaker = self._find_speaker_at_time(diarization_segments, mid_sec)

            # í™”ì ë¼ë²¨ ì¶”ê°€
            speaker_label = f"[{speaker}]" if speaker else "[ì•Œ ìˆ˜ ì—†ìŒ]"

            merged_segments.append((srt_start, srt_end, speaker_label, text))

        logger.info(f"âœ… ë³‘í•© ì™„ë£Œ: {len(merged_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

        return merged_segments

    @staticmethod
    def _timestamp_to_seconds(timestamp: str) -> float:
        """
        SRT íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜

        Args:
            timestamp: SRT íƒ€ì„ìŠ¤íƒ¬í”„ (HH:MM:SS,mmm)

        Returns:
            ì´ˆ ë‹¨ìœ„ ì‹œê°„
        """
        time_part, ms_part = timestamp.split(",")
        hours, minutes, seconds = map(int, time_part.split(":"))
        milliseconds = int(ms_part)

        return hours * 3600 + minutes * 60 + seconds + milliseconds / 1000

    @staticmethod
    def _find_speaker_at_time(
        diarization_segments: List[Tuple[float, float, str]], time_sec: float
    ) -> str:
        """
        íŠ¹ì • ì‹œê°ì˜ í™”ì ì°¾ê¸°

        Args:
            diarization_segments: [(ì‹œì‘, ì¢…ë£Œ, í™”ì), ...]
            time_sec: ì°¾ì„ ì‹œê° (ì´ˆ)

        Returns:
            í™”ì ID
        """
        for start, end, speaker in diarization_segments:
            if start <= time_sec <= end:
                return speaker

        return "UNKNOWN"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
diarization_service = DiarizationService()
