"""
Ollama LLM ì„œë¹„ìŠ¤
Ollama REST APIë¥¼ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±
"""
from pathlib import Path
from typing import Optional

import httpx
from loguru import logger

from app.core.config import settings


class OllamaService:
    """Ollama LLM ì„œë¹„ìŠ¤"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.base_url = settings.ollama_base_url
        self.model = settings.ollama_model
        self.timeout = settings.ollama_timeout

    async def check_health(self) -> bool:
        """
        Ollama ì„œë²„ ìƒíƒœ í™•ì¸

        Returns:
            ì„œë²„ ê°€ìš© ì—¬ë¶€
        """
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                return response.status_code == 200
        except Exception as e:
            logger.error(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    async def summarize(
        self,
        transcript: str,
        prompt_template: Optional[str] = None,
        dictionary_content: Optional[str] = None,
    ) -> str:
        """
        ëŒ€í™” ë‚´ìš© ìš”ì•½

        Args:
            transcript: ëŒ€í™” ì „ë¬¸ (SRT í˜•ì‹ ë˜ëŠ” í…ìŠ¤íŠ¸)
            prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            dictionary_content: ìš©ì–´ ì‚¬ì „ ë‚´ìš©

        Returns:
            ìš”ì•½ í…ìŠ¤íŠ¸
        """
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        if prompt_template is None:
            prompt_file = settings.config_dir / "default_prompt.txt"
            prompt_template = prompt_file.read_text(encoding="utf-8")

        # ìš©ì–´ ì‚¬ì „ ë¡œë“œ
        if dictionary_content is None:
            dict_file = settings.config_dir / "dictionary.txt"
            if dict_file.exists():
                dictionary_content = dict_file.read_text(encoding="utf-8")
            else:
                dictionary_content = ""

        # ìš©ì–´ ì‚¬ì „ ì„¹ì…˜ ìƒì„±
        if dictionary_content.strip():
            dictionary_section = f"ìš©ì–´ì‚¬ì „:\n{dictionary_content}\n"
        else:
            dictionary_section = ""

        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        full_prompt = prompt_template.format(
            dictionary_section=dictionary_section,
            transcript_text=transcript,
        )

        logger.info("ğŸ¤– LLM ìš”ì•½ ì‹œì‘")
        logger.debug(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(full_prompt)} ë¬¸ì")

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ëœ ìš”ì•½ ìƒì„±
                            "top_p": 0.9,
                            "num_predict": 512,  # ìµœëŒ€ í† í° ìˆ˜
                        },
                    },
                )

                response.raise_for_status()
                result = response.json()

                summary = result.get("response", "").strip()

                logger.info(f"âœ… LLM ìš”ì•½ ì™„ë£Œ: {len(summary)} ë¬¸ì")
                logger.debug(f"ìš”ì•½ ë‚´ìš©: {summary[:100]}...")

                return summary

        except httpx.TimeoutException:
            logger.error(f"âŒ LLM ìš”ì•½ íƒ€ì„ì•„ì›ƒ ({self.timeout}ì´ˆ)")
            raise

        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ LLM API ì—ëŸ¬: {e.response.status_code} - {e.response.text}")
            raise

        except Exception as e:
            logger.error(f"âŒ LLM ìš”ì•½ ì‹¤íŒ¨: {e}")
            raise

    def summarize_sync(
        self,
        transcript: str,
        prompt_template: Optional[str] = None,
        dictionary_content: Optional[str] = None,
    ) -> str:
        """
        ëŒ€í™” ë‚´ìš© ìš”ì•½ (ë™ê¸° ë²„ì „, Celery íƒœìŠ¤í¬ìš©)

        Args:
            transcript: ëŒ€í™” ì „ë¬¸
            prompt_template: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            dictionary_content: ìš©ì–´ ì‚¬ì „ ë‚´ìš©

        Returns:
            ìš”ì•½ í…ìŠ¤íŠ¸
        """
        import asyncio

        # ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë° ì‹¤í–‰
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        return loop.run_until_complete(
            self.summarize(transcript, prompt_template, dictionary_content)
        )


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ollama_service = OllamaService()
