# Voicecom AI - ìŒì„± íŒŒì¼ ìë™ ì²˜ë¦¬ ì‹œìŠ¤í…œ

WAV ìŒì„± íŒŒì¼ì„ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜(STT) í›„ LLM ê¸°ë°˜ ìš”ì•½ ìƒì„± ì‹œìŠ¤í…œ

## ğŸ“‹ ì‹œìŠ¤í…œ ê°œìš”

**ê¸°ëŠ¥**: Whisper STT â†’ SRT ìƒì„± â†’ LLM ìš”ì•½ â†’ ìë™ íŒŒì¼ ê´€ë¦¬

**ì²˜ë¦¬ íë¦„**:
```
input/
  â†“ (ìë™ ê°ì§€)
WAV íŒŒì¼ â†’ STT (Whisper) â†’ SRT ìƒì„±
  â†“
LLM ìš”ì•½ (Ollama midm-2.0)
  â†“
output/ (SRT + ìš”ì•½.txt)
  â†“
processed/ (ì›ë³¸ ì´ë™)
```

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

| ê³„ì¸µ | ê¸°ìˆ  | ìš©ë„ |
|------|------|------|
| Web API | FastAPI + Uvicorn | REST API ì„œë²„ |
| Task Queue | Celery + Redis | ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ |
| STT | faster-whisper | Whisper large-v3-turbo |
| LLM | Ollama | midm-2.0:base |
| DB | SQLite | ì‘ì—… ìƒíƒœ ê´€ë¦¬ |
| GPU | CUDA 12.x | GPU ê°€ì† |

## ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

### 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- Windows 10/11 ë˜ëŠ” Windows Server 2019/2022
- NVIDIA GPU (VRAM 12GB ì´ìƒ)
- CUDA 12.x ì„¤ì¹˜
- Python 3.10 ì´ìƒ

### 2. í™˜ê²½ ì„¤ì •

```bash
# 1. ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™” (Windows)
venv\Scripts\activate

# 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. Redis ì„¤ì¹˜

**Windows Redis ë‹¤ìš´ë¡œë“œ**:
```bash
# Redis for Windows (Memurai ë˜ëŠ” Redis Windows Port)
# https://github.com/redis-windows/redis-windows/releases
```

ë˜ëŠ” Docker ì‚¬ìš©:
```bash
docker run -d -p 6379:6379 redis:latest
```

### 4. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# Ollama ì„¤ì¹˜ (https://ollama.ai)

# midm-2.0:base ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull midm-2.0:base
```

### 5. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env.example`ì„ `.env`ë¡œ ë³µì‚¬í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •:
```bash
cp .env.example .env
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### Windowsì—ì„œ ëª¨ë“  ì„œë¹„ìŠ¤ ì‹¤í–‰

`run_services.bat` íŒŒì¼ ì‹¤í–‰:
```batch
run_services.bat
```

ë˜ëŠ” ê°œë³„ ì‹¤í–‰:

**1. Redis ì‹œì‘** (ë³„ë„ í„°ë¯¸ë„)
```bash
redis-server
```

**2. Ollama ì‹œì‘** (ë³„ë„ í„°ë¯¸ë„)
```bash
ollama serve
```

**3. Celery Worker ì‹œì‘** (ë³„ë„ í„°ë¯¸ë„)
```bash
celery -A app.tasks.celery_app worker --loglevel=info --pool=solo
```

**4. FastAPI ì„œë²„ ì‹œì‘** (ë³„ë„ í„°ë¯¸ë„)
```bash
python app/main.py
```

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
voicecom_ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/              # REST API
â”‚   â”œâ”€â”€ core/             # ì„¤ì •
â”‚   â”œâ”€â”€ tasks/            # Celery íƒœìŠ¤í¬
â”‚   â”œâ”€â”€ services/         # Whisper, Ollama ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ utils/            # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ config/               # í”„ë¡¬í”„íŠ¸, ìš©ì–´ ì‚¬ì „
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/           # WAV ì…ë ¥ (ì—¬ê¸°ì— íŒŒì¼ ë„£ê¸°)
â”‚   â”œâ”€â”€ output/          # SRT + ìš”ì•½ ì¶œë ¥
â”‚   â”œâ”€â”€ processed/       # ì²˜ë¦¬ ì™„ë£Œ WAV
â”‚   â””â”€â”€ error/           # ì—ëŸ¬ íŒŒì¼
â””â”€â”€ logs/                # ë¡œê·¸
```

## ğŸ’» API ì‚¬ìš©ë²•

### ì„œë²„ ì‹¤í–‰ í›„ ì ‘ì†

- **API ë¬¸ì„œ**: http://localhost:8000/docs
- **ê¸°ë³¸ URL**: http://localhost:8000/api/v1

### ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸

#### 1. íŒŒì¼ ì—…ë¡œë“œ
```bash
POST /api/v1/upload
Content-Type: multipart/form-data

íŒŒë¼ë¯¸í„°:
  file: WAV íŒŒì¼

ì‘ë‹µ:
{
  "task_id": "uuid",
  "filename": "example.wav",
  "status": "pending"
}
```

#### 2. ì‘ì—… ìƒíƒœ ì¡°íšŒ
```bash
GET /api/v1/tasks/{task_id}

ì‘ë‹µ:
{
  "task_id": "uuid",
  "filename": "example.wav",
  "status": "in_progress",
  "progress": 50
}
```

#### 3. í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
```bash
PUT /api/v1/config/prompt
Content-Type: application/json

{
  "prompt_content": "ìƒˆ í”„ë¡¬í”„íŠ¸ ë‚´ìš©..."
}
```

## ğŸ¯ ì²˜ë¦¬ íë¦„ ìƒì„¸

### Mono íŒŒì¼ ì²˜ë¦¬

```
mono.wav (input/)
  â†“
Whisper STT â†’ mono.srt
  â†“
LLM ìš”ì•½ â†’ mono_ìš”ì•½.txt
  â†“
output/ ì €ì¥ + processed/ ì´ë™
```

### Stereo íŒŒì¼ ì²˜ë¦¬

```
stereo.wav (input/)
  â†“
ì±„ë„ ë¶„ë¦¬ â†’ stereo_1ch.wav + stereo_2ch.wav
  â†“
ê° ì±„ë„ STT â†’ [í™”ì1], [í™”ì2] ë¼ë²¨ë§
  â†“
í†µí•© SRT ìƒì„± â†’ stereo.srt
  â†“
LLM ìš”ì•½ â†’ stereo_ìš”ì•½.txt
  â†“
output/ ì €ì¥ + processed/ ì´ë™
```

## ğŸ”§ ì„¤ì • íŒŒì¼

### config/default_prompt.txt
LLM ìš”ì•½ì— ì‚¬ìš©ë˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

### config/dictionary.txt
ìš©ì–´ ì‚¬ì „ (ì§€ì—­ëª…, ë³‘ì›ëª…, ì „ë¬¸ìš©ì–´)

## âš™ï¸ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

**VRAM 12GB í™˜ê²½**:
- Whisper: 2-4GB
- Ollama: 4-6GB
- ì—¬ìœ  ê³µê°„: 2-4GB

**ì²˜ë¦¬ ì „ëµ**:
1. Whisper ì²˜ë¦¬ â†’ GPU ë©”ëª¨ë¦¬ í•´ì œ
2. Ollama ìš”ì•½ ì‹œì‘
3. ìˆœì°¨ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì•ˆì •ì„± í™•ë³´

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Redis ì—°ê²° ì‹¤íŒ¨
```bash
# Redis ì‹¤í–‰ í™•ì¸
redis-cli ping
# ì‘ë‹µ: PONG
```

### 2. Ollama ì—°ê²° ì‹¤íŒ¨
```bash
# Ollama ìƒíƒœ í™•ì¸
ollama list
```

### 3. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# settings ìˆ˜ì •
GPU_MEMORY_RESERVE_MB=2048  # ë” ë§ì€ ì—¬ìœ  ê³µê°„ í™•ë³´
```

### 4. Celery Worker ì—ëŸ¬ (Windows)
```bash
# --pool=solo ì˜µì…˜ í•„ìˆ˜ (Windows gevent ì´ìŠˆ)
celery -A app.tasks.celery_app worker --pool=solo
```

## ğŸ“ ë¡œê·¸ í™•ì¸

```bash
# ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜
logs/app.log

# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸ (Linux/Mac)
tail -f logs/app.log

# Windows PowerShell
Get-Content logs/app.log -Wait
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/
```

## ğŸ“Š Phase 1 ì™„ë£Œ ìƒíƒœ

âœ… í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
âœ… FastAPI REST API
âœ… Celery + Redis ì„¤ì •
âœ… Whisper STT ì„œë¹„ìŠ¤
âœ… Ollama LLM ìš”ì•½
âœ… Mono íŒŒì¼ ì²˜ë¦¬
âœ… Stereo íŒŒì¼ ì²˜ë¦¬ (ì±„ë„ ë¶„ë¦¬)
âœ… ì—ëŸ¬ í•¸ë“¤ë§

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (Phase 2-4)

- [ ] Watchdog íŒŒì¼ ìë™ ê°ì§€
- [ ] Tkinter GUI
- [ ] ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
- [ ] PyInstaller EXE ë¹Œë“œ
- [ ] ì„¤ì¹˜ íŒ¨í‚¤ì§€ ìƒì„±

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ë‚´ë¶€ í”„ë¡œì íŠ¸ (ë¹„ê³µê°œ)
# aicc
